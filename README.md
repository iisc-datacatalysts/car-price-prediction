# Predictive Modelling for Used Car Pricing

A comprehensive machine learning project for predicting used car prices using various regression models and advanced feature engineering techniques.

## üìã Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
  - [Running the Notebook](#running-the-notebook)
  - [Making Predictions](#making-predictions)
  - [Interactive Price Prediction Dashboard](#interactive-price-prediction-dashboard)
- [Methodology](#methodology)
- [Models Evaluated](#models-evaluated)
- [Results](#results)
- [Technologies Used](#technologies-used)
- [Key Insights](#key-insights)
- [Future Improvements](#future-improvements)

## üéØ Overview

This project implements an end-to-end machine learning pipeline for predicting used car prices in the Indian market. The solution includes:

> **üöÄ Try the Interactive Dashboard**: Run the notebook and launch the Gradio interface (Cells 79-82) to get a shareable public URL for the price prediction dashboard. The URL will be displayed in the notebook output when you run Cell 82.

- **Comprehensive Data Cleaning**: Handling missing values, normalizing units, and extracting features from mixed-format columns
- **Enhanced Preprocessing**: 
  - **KNN Imputation**: Using K-Nearest Neighbors imputation (n_neighbors=5) for sophisticated missing value handling
  - **Polynomial Features**: Generating interaction features (degree=2, interaction_only=True) to capture feature relationships
  - Standard scaling for numerical features
- **Advanced Feature Engineering**: Creating derived features like car age, km/year, frequency encodings, and residual encodings
- **Multiple Model Evaluation**: Comparing baseline and advanced models including Ridge, Random Forest, XGBoost, CatBoost, and LightGBM
- **Target Encoding**: Using target encoding for high-cardinality categorical features
- **Quantile Regression**: Providing prediction intervals for uncertainty estimation
- **Model Interpretability**: SHAP analysis for feature importance
- **Production-Ready Inference**: Serialized models and inference function for deployment
- **Interactive Dashboard**: Gradio-based web interface for real-time price predictions

## üìä Dataset

The dataset contains information about **8,128 used cars** with the following attributes:

### Features

**Car Identification & Basic Info:**
- `name`: Full car name (combines brand and model)
- `year`: Manufacturing year
- `selling_price`: Target variable - the price at which the car was sold

**Car Usage History:**
- `km_driven`: Total kilometers driven
- `owner`: Ownership history (First Owner, Second Owner, etc.)
- `seller_type`: Type of seller (Individual, Dealer, Trustmark Dealer)

**Car Specifications:**
- `fuel`: Fuel type (Diesel, Petrol, LPG, CNG)
- `transmission`: Transmission type (Manual, Automatic)
- `mileage`: Fuel efficiency (with units: kmpl or km/kg)
- `engine`: Engine displacement (in CC)
- `max_power`: Maximum power output (in bhp)
- `torque`: Torque specification (in Nm or kgm, with RPM range)
- `seats`: Number of seats

**Dataset Source:** The dataset is loaded from the IISC Data Catalysts GitHub repository. The original dataset is maintained by Kaggle team - [Kaggle](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho). This has been downloaded from Kaggle and uploaded into GitHub for ease of access.

## üîß Features

### Data Preprocessing
- **KNN Imputation**: Uses K-Nearest Neighbors (n_neighbors=5) to impute missing values based on similar observations (more sophisticated than median imputation)
- Missing value imputation (constant for categorical)
- Unit normalization (km/kg ‚Üí kmpl, kgm ‚Üí Nm)
- String parsing for mixed-format columns (mileage, engine, max_power, torque)
- RPM extraction from complex torque strings
- **Polynomial Features**: Generates interaction features (degree=2, interaction_only=True) to capture multiplicative relationships between features
- Standard scaling for numerical features

### Feature Engineering
- **Derived Features**: Car age, kilometers per year, abnormal usage flags
- **Frequency Encoding**: Normalized frequency of make and model occurrences
- **Residual Encoding**: Model-specific price deviations from overall mean
- **Log Transformation**: Applied to target variable to handle right-skewed distribution

### Model Training
- Stratified train-test split (85% train, 15% test)
- Cross-validation with honest target encoding
- Early stopping for gradient boosting models
- Hyperparameter tuning for optimal performance

## üìÅ Project Structure

```
car-price-prediction/
‚îú‚îÄ‚îÄ DSPCourse_Project_UsedCarPricePrediction.ipynb  # Main notebook
‚îú‚îÄ‚îÄ README.md                                       # This file
‚îú‚îÄ‚îÄ CarData.csv                                     # Dataset (can be loaded from GitHub)
‚îî‚îÄ‚îÄ artifacts/                                      # Saved models (generated after running)
    ‚îú‚îÄ‚îÄ preprocessor_lgb.joblib                    # Preprocessing pipeline
    ‚îú‚îÄ‚îÄ target_encoder.joblib                       # Target encoder
    ‚îú‚îÄ‚îÄ lgb_model.txt                               # Main LightGBM model
    ‚îî‚îÄ‚îÄ lgb_quantile_*.txt                          # Quantile regression models
```

## üöÄ Installation

### Prerequisites
- Python 3.10 or higher
- Jupyter Notebook or JupyterLab

### Setup

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd DSP
   ```

2. **Install required packages:**
   
   The notebook will automatically install packages in the first cell, or you can install them manually:
   
   ```bash
   pip install shap lightgbm category_encoders catboost scikit-learn pandas numpy matplotlib seaborn gradio
   ```
   
   Note: Gradio is installed automatically when you run the interactive dashboard cells (Cell 80).

3. **Open the notebook:**
   ```bash
   jupyter notebook DSPCourse_Project_UsedCarPricePrediction.ipynb
   ```

## üíª Usage

### Running the Notebook

1. Open the notebook in Jupyter
2. Run all cells sequentially (Cell ‚Üí Run All)
3. The notebook will:
   - Load data from GitHub
   - Perform data cleaning and feature engineering
   - Train multiple models
   - Evaluate performance
   - Save model artifacts to `artifacts/` directory

### Making Predictions

After training, you can use the `predict_single()` function to predict prices for new cars:

```python
# Example usage
example_car = {
    'name': 'Maruti Swift Dzire VDI',
    'year': 2014,
    'kms_driven': 145500,
    'fuel': 'Diesel',
    'transmission': 'Manual',
    'owner': 'First Owner',
    'mileage': '23.4 kmpl',
    'engine': '1248 CC',
    'max_power': '74 bhp',
    'torque': '190Nm@ 2000rpm',
    'seats': 5.0
}

predicted_price = predict_single(example_car)
print(f"Predicted Price: ‚Çπ{predicted_price:,.0f}")
```

**Note:** `seller_type` is present in the dataset but is not used as a feature in the model. The model uses `fuel`, `transmission`, and `owner` as categorical features. You can omit `seller_type` from the input dictionary.

### Interactive Price Prediction Dashboard

The notebook includes an interactive Gradio web interface for easy price predictions. The dashboard is embedded directly in the notebook and provides a user-friendly way to estimate car prices.

**To use the dashboard:**

1. **Run all previous cells** (especially model training cells) to ensure the model is trained
2. **Run Cell 79-82** to launch the Gradio interface:
   - Cell 79: Markdown header for the dashboard section
   - Cell 80: Installs Gradio library
   - Cell 81: Defines helper functions for parsing and feature engineering
   - Cell 82: Creates and launches the Gradio interface

**Dashboard Features:**
- **Two-column layout**: Required fields on the left, optional fields on the right
- **Input validation**: Checks required fields before prediction
- **Example inputs**: 3 pre-filled examples you can click to auto-fill the form
- **Real-time predictions**: Instant price estimates with formatted display
- **Public URL**: Automatically generates a shareable link (works great in Google Colab)
- **Embedded interface**: Displays directly in the notebook output

**Accessing the Dashboard URL:**

When you run Cell 82, Gradio automatically generates a public URL that will be displayed in the notebook output. The URL format is typically:
- **Public URL**: `https://xxxxx.gradio.live` (shareable, expires in 1 week)
- **Local URL**: `http://127.0.0.1:7860` (works when running locally in Jupyter)

**Example Output:**
```
Running on local URL:  http://127.0.0.1:7860
Running on public URL: https://xxxxx.gradio.live

This share link expires in 1 week. For free permanent hosting and GPU upgrades, 
run `gradio deploy` from the terminal to deploy to Hugging Face Spaces.
```

**Important Notes:**
- The public URL is automatically generated and shareable with anyone
- Free Gradio URLs expire after 1 week
- A new URL is generated each time you run the dashboard
- For permanent hosting, deploy to [Hugging Face Spaces](https://huggingface.co/spaces) using `gradio deploy`
- The interface is also embedded directly in the notebook output

**Required Fields:**
- Car Name (e.g., "Maruti Swift Dzire VDI")
- Manufacturing Year
- Kilometers Driven
- Fuel Type (Diesel, Petrol, LPG, CNG)
- Transmission (Manual, Automatic)
- Owner (First Owner, Second Owner, etc.)

**Optional Fields:**
- Mileage (format: "23.4 kmpl" or "17.3 km/kg")
- Engine (format: "1248 CC")
- Max Power (format: "74 bhp")
- Torque (format: "190Nm@ 2000rpm")
- Number of Seats

**Note:** The dashboard uses the trained model variables directly from the notebook's namespace, so make sure all model training cells have been executed first.

**Accessing the Dashboard URL:**

When you run Cell 82, Gradio will automatically generate a public URL. The URL will be displayed in the notebook output, for example:

```
Running on local URL:  http://127.0.0.1:7860
Running on public URL: https://1152680073981e44ef.gradio.live

This share link expires in 1 week. For free permanent hosting and GPU upgrades, 
run `gradio deploy` from the terminal to deploy to Hugging Face Spaces.
```

**Important Notes:**
- The public URL is automatically generated and shareable
- Free Gradio URLs expire after 1 week
- The URL is unique each time you run the dashboard
- For permanent hosting, deploy to [Hugging Face Spaces](https://huggingface.co/spaces) using `gradio deploy`
- The interface also works locally at `http://127.0.0.1:7860` when running in Jupyter
- In Google Colab, the public URL allows access from anywhere

## üìà Methodology

### Data Cleaning Pipeline

1. **Name Parsing**: Extract make (brand) and model from car names
2. **Mileage Normalization**: Convert km/kg to kmpl using fuel density ratios
3. **Torque Processing**: Extract torque values and normalize units (kgm ‚Üí Nm)
4. **RPM Extraction**: Parse RPM information from various formats (ranges, +/- notation)

### Enhanced Preprocessing Pipeline

1. **KNN Imputation**: Uses K-Nearest Neighbors (n_neighbors=5) to impute missing values. This is more sophisticated than median imputation as it considers relationships between features to provide more accurate missing value estimates.
2. **Standard Scaling**: Normalizes features to have zero mean and unit variance
3. **Polynomial Features**: Creates interaction features (degree=2, interaction_only=True) to capture multiplicative relationships between features (e.g., age √ó kms_driven, age √ó max_power_value). The `interaction_only=True` parameter means only interaction terms are created, not squared terms.

### Feature Engineering

1. **Temporal Features**: Calculate car age from manufacturing year
2. **Usage Features**: Compute kilometers per year and flag abnormal usage
3. **Encoding Features**: 
   - Frequency encoding for make/model popularity
   - Residual encoding for model-specific price patterns
4. **Target Transformation**: Apply log1p transformation to handle skewness

### Model Training Strategy

1. **Baseline Models**: Start with simple linear models (Ridge) with enhanced preprocessing (KNN imputation + polynomial features)
2. **Tree-Based Models**: Evaluate Random Forest, XGBoost, CatBoost, LightGBM
3. **Ensemble Methods**: Stack multiple models for improved performance
4. **Advanced Techniques**: 
   - Target encoding for high-cardinality features (make, model)
   - Quantile regression for uncertainty estimation
   - Cross-validation with honest target encoding (out-of-fold predictions)
   - Early stopping for gradient boosting models

## ü§ñ Models Evaluated

| Model | Description | Key Features |
|-------|-------------|--------------|
| **Ridge Regression** | Linear baseline model | L2 regularization, fast training |
| **Random Forest** | Ensemble of decision trees | Non-linear relationships, feature interactions |
| **XGBoost** | Gradient boosting framework | Regularization, subsampling |
| **CatBoost** | Categorical boosting | Native categorical handling |
| **LightGBM** | Gradient boosting framework | Fast training, low memory usage |
| **Stacking Ensemble** | Meta-learner combining models | Combines predictions from multiple models |
| **LightGBM + Target Encoding** | Advanced model with encoding | Best performance, handles high-cardinality features |

## üìä Results

### Model Performance

The LightGBM model with target encoding achieved the best performance:

- **Training R¬≤**: 0.984
- **Test R¬≤**: 0.950
- **RMSE**: ‚Çπ102,484
- **MAE**: ‚Çπ57,977

### Cross-Validation Results

5-fold cross-validation R¬≤ scores:
- **Ridge Regression**: 0.84 - 0.94 (varies by fold)
- **CatBoost**: 0.94 - 0.95
- **LightGBM**: 0.94 - 0.95
- **XGBoost**: 0.93 - 0.95

### Key Findings

1. **Strong Predictors**:
   - Car age (negative correlation: -0.71)
   - Kilometers driven (negative correlation: -0.25)
   - Max power and engine size (positive correlations)

2. **Categorical Insights**:
   - Diesel cars command higher prices than Petrol/LPG/CNG
   - Automatic transmission increases price
   - First owner cars are most valuable

3. **Prediction Intervals**:
   - 90% prediction interval coverage: ~77.0%
   - Quantile regression provides uncertainty estimates (5th, 50th, 95th percentiles)

## üõ† Technologies Used

- **Python 3.10+**
- **Data Science Libraries**:
  - `pandas` - Data manipulation
  - `numpy` - Numerical computing
  - `scikit-learn` - Machine learning
- **Advanced ML Libraries**:
  - `lightgbm` - Gradient boosting
  - `xgboost` - Gradient boosting
  - `catboost` - Categorical boosting
  - `category_encoders` - Target encoding
- **Visualization**:
  - `matplotlib` - Plotting
  - `seaborn` - Statistical visualization
  - `shap` - Model interpretability
- **Interactive UI**:
  - `gradio` - Web interface for model predictions
- **Utilities**:
  - `joblib` - Model serialization

## üîç Key Insights

1. **Data Quality**: The dataset contains missing values in performance attributes (~221 missing in mileage, engine, max_power, torque, seats), which are handled through KNN imputation (n_neighbors=5) - a more sophisticated approach than median imputation

2. **Feature Importance**: Car age is the strongest predictor, followed by kilometers driven and engine specifications

3. **Model Selection**: LightGBM with target encoding outperforms other models, balancing accuracy and training time

4. **Target Transformation**: Log transformation is crucial for handling the right-skewed price distribution

5. **High-Cardinality Features**: Target encoding effectively handles the 2,058 unique car names and models

## üîÆ Future Improvements

- **Enhanced Name Parsing**: Use regex patterns or fuzzy matching against curated make/model databases for more accurate extraction
- **Leakage Testing**: Evaluate models with and without potentially leaked features to ensure robustness
- **Calibrated Intervals**: Implement conformalized quantile regression for better-calibrated uncertainty estimates
- **Additional Features**: Explore brand reputation scores, depreciation curves, market segment indicators
- **Polynomial Feature Selection**: Consider feature selection techniques to reduce dimensionality after polynomial feature generation
- **KNN Imputation Tuning**: Experiment with different values of n_neighbors for KNN imputation to optimize missing value handling
- **Model Optimization**: Hyperparameter tuning with Optuna or similar tools
- **Testing**: Add unit tests for preprocessing pipeline and inference function before production deployment
- **Computational Efficiency**: Monitor training time with polynomial features; consider feature selection if dimensionality becomes too high
- **Deployment**: Create API endpoint for real-time predictions
- **Dashboard Enhancements**: Add prediction intervals, feature importance visualization, and comparison with similar cars

## üìù Notes

- The dataset is automatically loaded from the IISC Data Catalysts GitHub repository
- Python 3.10+ is recommended for running this notebook
- Required packages are installed in the first code cell of the notebook
- Gradio is automatically installed when running the interactive dashboard cells
- All models are saved in the `artifacts/` directory after training
- The notebook includes comprehensive documentation and comments
- The enhanced preprocessing pipeline uses KNN imputation and polynomial interaction features
- The interactive Gradio dashboard is embedded in the notebook and works seamlessly in both Jupyter and Google Colab
- Results may vary slightly due to random seeds and data splits

## üë§ Authors

- **Vimalraj Kanagaraj** (vimalrajk@iisc.ac.in)
- **Anfaal Obaid Waafy** (anfaalwaafy@iisc.ac.in)
- **Manikanda Sakthi Subramaniam**(manikandasa1@iisc.ac.in)
- **Abhilasha Kawle**(abhilashak@iisc.ac.in)

## Dataset: [Kaggale](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho)

## üìÑ License

This project is part of a course assignment. Please refer to the course guidelines for usage terms.

## üôè Acknowledgments

- Dataset provided via GitHub repository
- Course instructors and materials
- Open-source ML community for excellent libraries

---

**Note**: This project is for educational purposes. Model performance may vary with different datasets or market conditions.

